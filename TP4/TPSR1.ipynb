{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thad75/OptionAI/blob/RecSys/TP4/TPSR1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5mBUDgUqXQH"
      },
      "outputs": [],
      "source": [
        "!pip install spotipy\n",
        "!pip install pytorch_lightning\n",
        "import spotipy\n",
        "from spotipy.oauth2 import SpotifyClientCredentials\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import scipy.sparse as sp\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import pytorch_lightning as pl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TP3 : Spotify Recommendation System\n",
        "\n",
        "As a future Deep Learning engineer, you will be asked to tackle some fundamental tasks. Recommendation Systems is one of these fundamental task. Obviously, you had a course introducing you to the world of recommendation. Hence, we will create a recommendation system based on Spotify Data. \n",
        "\n",
        "**The goal is to create a Playlist recommendation system.**\n"
      ],
      "metadata": {
        "id": "tFX0OEUpqaFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outline\n",
        "\n",
        "Goal of this lab:\n",
        "* Use an API\n",
        "* **Read Documentation** \n",
        "* Create a functioning model\n",
        "* Use Pytorch-Lightning\n",
        "* See all the roles a Data Scientist can take\n",
        "\n",
        "This lab is written to introduce you to a real-case application of RecSys\n",
        "\n"
      ],
      "metadata": {
        "id": "5ozZczBb28xA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Important : Set your Credentials \n",
        "\n",
        "Before beginning the lab make sure that you set your Spotify API Credentials correctly. If you don't have any credentials, please refer to TDm 1 and retrieve your keys."
      ],
      "metadata": {
        "id": "MgSXez8Ii6vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Set your credentials\n",
        "id = ...\n",
        "secret = ...\n",
        "# Set up Spotipy client credentials\n",
        "client_credentials_manager = SpotifyClientCredentials(client_id=id, client_secret=secret)\n",
        "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)"
      ],
      "metadata": {
        "id": "P2BjD4BMjHvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "You are working as a Data Scientist in FlexCorp, a startup that makes a Recommendation System. Obviously, as any flexible companies, an Intern was hired and did some stuff. And obviouly, there are loads of errors in it. Let's correct and give a better product.\n",
        "\n",
        "You will be roaming around the different roles a Data Scientist can have in company. Your mission revolves around two things:\n",
        "* Correct the existing works\n",
        "* Fullfill the specification of the company\n",
        "\n",
        "The company uses Pytorch-Lightning as their DL Framework"
      ],
      "metadata": {
        "id": "Xzydofx_Y5HU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I - Extract, Transform & Load : Being a Data Engineer and Data Analyst\n",
        "\n",
        "Well, let's explore some data from Spotify. Obviously, we need some data. The intern did do some work on it. Let's check the usefulness and correct the errors.\n",
        "\n",
        "* Code Review: Correct the intern's work\n"
      ],
      "metadata": {
        "id": "lIr3HKiLjhKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A - Data Engineering : Retrieving and Cleaning Data \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Company's specification:\n",
        "* Retrieve Data from 1960 to 2023 in a list.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Kw_SYMAFYZDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Correct the intern's code \n",
        "\n",
        "query_res = [sp.search(q='year:' + str(i), type='track', limit=50) for i in range(2022, 2023)]\n",
        "tracks = []\n",
        "for result in query_res:\n",
        "    tracks += result['tracks']['items']\n",
        "print(tracks)"
      ],
      "metadata": {
        "id": "d57_KR2kXVpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "Company's specification:\n",
        "* Retrieve the following keys is a dictionnary:\n",
        "  * user_id\n",
        "  * song_id\n",
        "  * danceability\n",
        "  * energy\n",
        "  * speechiness\n",
        "  * loudness\n",
        "  * tempo\n",
        "\n",
        "* Create a pandas dataframe\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "PqEAjTwS8nRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "track = sp.audio_features(tracks[0]['id'])[0]\n",
        "track['tempo']"
      ],
      "metadata": {
        "id": "gE1ihoGBoJv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Extract relevant features from tracks\n",
        "data = []\n",
        "for i in range(len(tracks)):\n",
        "    track = sp.audio_features(tracks[i]['id'])[0]\n",
        "    # Hint : verify the existence of the keys in track[i]\n",
        "    data.append({\n",
        "        'user_id': 'user_{}'.format(i % 10),\n",
        "        'song_id': tracks[i]['id'],\n",
        "        'danceability': track['danceness'],\n",
        "        'energy': track['red bull'],\n",
        "        'speechiness': track['je mapelle JP'],\n",
        "        'loudness': track['loudness'],\n",
        "        'tempo': int(track['1+1 = 42'])\n",
        "    })\n",
        "\n",
        "# TODO : Create a dataset\n",
        "df = ..."
      ],
      "metadata": {
        "id": "5z24Z0mvfC5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* What do you think of the Intern's work ? Is he hireable ?"
      ],
      "metadata": {
        "id": "Ip7PwvTEcgwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B - Data Analyst : Plotting Some Stuffs to Understand the Data\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gX5YJUghZi6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data : Speak to Me !\n",
        "\n",
        "So we have a dataframe, but before training any models, we need to understand the data. \n",
        "\n",
        "Let's perform Exploratory Data Analysis\n",
        "* Perform the Data Exploration"
      ],
      "metadata": {
        "id": "KOjMuRMlNiJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Print your dataset\n"
      ],
      "metadata": {
        "id": "wzuOHiYz_28L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : What informations does this code gives you ?\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "kuCl50YdCYSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Print the unique songs and users.\n",
        "num_users = ...\n",
        "num_songs = ...\n",
        "print('Number of unique users:', num_users)\n",
        "print('Number of unique songs:', num_songs)"
      ],
      "metadata": {
        "id": "L6XnOWZVZppL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Is there any outliers in the DataFrame ? Check for nan values\n",
        "print(...)"
      ],
      "metadata": {
        "id": "D9dtZohZaXo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Plot the distribution of each features. \n",
        "df.hist(figsize=(10,10))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HopTAZbeaCKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Using Seaborn plot the pairwise distributions of each features.\n",
        "sns.pairplot(df[...])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RT-vIrJ1aiX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Plot the top 10 songs features sorted by their 'danceability'. Hint : GroupBy is your friend.\n",
        "top_songs = ...\n",
        "print(top_songs)\n"
      ],
      "metadata": {
        "id": "NgxkfmzIaJyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Plot the top 10 users feautures sorted by their danceab\n",
        "top_users = ...\n",
        "print(top_users)\n"
      ],
      "metadata": {
        "id": "UlzgrIthacyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normally, we are supposed to have a lot more understanding of the Dataset now. But there are few issues that might come from the data as is.\n",
        "* What further steps do we need on the dataset ?"
      ],
      "metadata": {
        "id": "bR8cruTsNNwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Rating\n",
        "\n",
        "As seen before, there's no specific ratings here. We can create one based on the other features. But we need to preprocess a little bit the existing features.\n",
        "* Do we need to perform preprocessing on each features ?\n",
        "* What type of preprocessing do we need to perform on the selected features ?\n",
        "* What should it change on the data ?\n",
        "* Using MinMaxScale, do the needed preprocessing.\n",
        "\n",
        "More documentation on :  https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n"
      ],
      "metadata": {
        "id": "fHo280yKa_P4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "# TODO : Define the columns to normalize. Hint : It must be a list of strings\n",
        "cols_to_normalize = [...]\n",
        "# TODO : Rescale and Replace the values apres preprocessing.\n",
        "df[cols_to_normalize] = scaler.fit_transform(df[cols_to_normalize])\n"
      ],
      "metadata": {
        "id": "z_9rKAkZbBLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can compute a rating based on other features. For the moment, we will just take in account 2 features for the computation of the rating value:   **Energy, Loudness**\n",
        "\n",
        "* When computing the rating value, do we need to re-do some post-processing on it ?\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "9-XLJdM7PJGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Create your ratings\n",
        "df['rating'] = ...\n"
      ],
      "metadata": {
        "id": "TS8NlgdWbEjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Using .describe(), describe your new dataframe. What can you tell about the rating column ?\n",
        "..."
      ],
      "metadata": {
        "id": "3I9Wv_Dmbid9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Calculate the mean rating for each user. Don't forget to groupby\n",
        "user_ratings = ...\n",
        "\n",
        "# TODO : Calculate the mean rating for each song\n",
        "song_ratings = ...\n"
      ],
      "metadata": {
        "id": "EngYHdmhbn_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data : Speak to the Computer !\n",
        "\n",
        "We're nearly over with the Data Engineering stuff. But we need to change something crucial about the existing user/items values.\n",
        "\n",
        "* What is the issue with the current form of the Song_id and User_id values ?\n",
        "* As seen in class, what preprocessing do we need to do on these specific columns ?\n",
        "* Do we need to create separate pre-processors to re-encode the user and song columns ? Why ?"
      ],
      "metadata": {
        "id": "xJKZGghlQXqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Print the user_id or song_id. What is the issue ?"
      ],
      "metadata": {
        "id": "8Tp9KUBrRALX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# TODO : Re-encode the user_id and song_id columns to the correct values.\n",
        "encoder_user = ...\n",
        "encoder_song = ...\n",
        "\n",
        "df.user_id = ...\n",
        "df.song_id = ..."
      ],
      "metadata": {
        "id": "lSraQSXa8zYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C - Creating the PyTorch Dataset: Being a ML/DL Ops\n",
        "\n"
      ],
      "metadata": {
        "id": "JCBySE2Rc3B3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating the PyTorch Dataset\n",
        "\n",
        "That was a long ride. But we are ready to train our model now. But first, let's create a Pytorch Dataset Class that we can reuse.\n",
        "The PlaylistDataset class takes as input:\n",
        "* df : the dataframe you've been creating from the beginning\n",
        "Now let's get into the details of this class : \n"
      ],
      "metadata": {
        "id": "eI3-tx1tTiTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Fill in the blanks"
      ],
      "metadata": {
        "id": "Ko8lsVf4ThZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PlaylistDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # TODO : Retrieve the row corresponding to idx from the dataframe\n",
        "        row = ...\n",
        "        # TODO : Retrieve these values and convert them into torch.tensor. Hint : Really be careful on the type of everything..\n",
        "        user = torch.tensor(row['user_id'],dtype=torch.int) # Example\n",
        "        song = ...\n",
        "        danceability = ...\n",
        "        energy = ...\n",
        "        speechiness = ...\n",
        "        loudness = ...\n",
        "        tempo = ...\n",
        "        rating = ...\n",
        "\n",
        "\n",
        "        return {\n",
        "            'user': user,\n",
        "            'song': song,\n",
        "            'danceability': danceability,\n",
        "            'energy': energy,\n",
        "            'speechiness': speechiness,\n",
        "            'loudness': loudness,\n",
        "            'tempo': tempo,\n",
        "            'rating': rating\n",
        "        }\n"
      ],
      "metadata": {
        "id": "ZnFD2JEZc7LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Make sure that your Dataset returns what we want.\n",
        "dataset = ...\n",
        "..."
      ],
      "metadata": {
        "id": "egRfGf8VSpJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating the PyTorch Lightning Datamodule\n",
        "\n",
        "Obviously, we can still rewrite all the code we wrote each and every time to train a model but meh, it's too much work :)\n",
        "\n",
        "Let's use Pytorch Lightning.\n",
        "\n",
        "* The DataModule class is a simple wrapper that encompasses all your datasets and dataloaders under one class. However, we need some proper initialization.\n",
        "We need to create the Training, Validation and Testing Dataloader for each data sets we have."
      ],
      "metadata": {
        "id": "iB28RDc_dpfd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Fill in the blanks"
      ],
      "metadata": {
        "id": "9XRKuXugUFQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PlaylistDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, df, batch_size=32, val_ratio=0.3):\n",
        "        super().__init__()\n",
        "        self.df = df\n",
        "        self.batch_size = batch_size\n",
        "        self.val_ratio = val_ratio\n",
        "        \n",
        "    def prepare_data(self):\n",
        "        # download or preprocess data here\n",
        "        pass\n",
        "        \n",
        "    def setup(self, stage=None):\n",
        "        if stage == 'fit' or stage is None:\n",
        "            # TODO : Split dataset into train and val sets\n",
        "            train_len = ...\n",
        "            val_len = ...\n",
        "            self.train_data, self.val_data = random_split(..., [train_len, val_len]) # TODO : Google Random_split, and find what you have to put here\n",
        "        if stage == 'test' or stage is None:\n",
        "            # use entire dataset for testing\n",
        "            self.test_data = self.df\n",
        "            \n",
        "    def train_dataloader(self):\n",
        "        # TODO : Init your train_loader\n",
        "        train_loader = DataLoader(..., batch_size=..., shuffle=True)\n",
        "        return train_loader\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        # TODO : Init your val_loader\n",
        "        val_loader = DataLoader(...,..., shuffle=False)\n",
        "        return val_loader\n",
        "    \n",
        "    def test_dataloader(self):\n",
        "        # TODO : Init your test_loader\n",
        "        test_loader = DataLoader(...,..., shuffle=False)\n",
        "        return test_loader\n"
      ],
      "metadata": {
        "id": "Wg8O-BQJeGAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II - Getting Back to the NCF with Lightning : Being a Data Scientist\n",
        "\n",
        "As seen in class, we have plenty of models available to train a RecSys model. Let's use a model everyone uses: Neural Collaborative Filtering Model.\n",
        "\n",
        "However, comparing it to the one presented in class, the present model is a little bit more complex version that will handle more features than the basic User/Items embedding layers of the NCF.\n",
        "* What are the two mains parts of the NCF model ?\n",
        "* What is the purpose of the nn.Embedding layer ?\n",
        "* What are the differences between the Embedding Layers and the Linear Layers.\n",
        "* What basic operation do we perform to keep the informations given by all the features with User Embedding and Item Embedding ?\n"
      ],
      "metadata": {
        "id": "nCUY4PPNjM5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "class NCF(pl.LightningModule):\n",
        "    def __init__(self, num_users, num_songs, embed_dim=32, hidden_dim=64, used_features=2):\n",
        "        super(NCF, self).__init__()\n",
        "        \n",
        "        # User embedding layer\n",
        "        self.user_embed = nn.Embedding(num_users, embed_dim)        \n",
        "        # TODO : Define your Song embedding layer\n",
        "        self.song_embed = ...\n",
        "        # TODO : Define your MLP layers. The first layer is given\n",
        "        self.layer1 = nn.Linear(embed_dim*2+used_features, hidden_dim)\n",
        "        self.layer2 = ...\n",
        "        self.layer3 = ...\n",
        "        \n",
        "        # TODO : What's the purpose of this class attribute ?\n",
        "        self.used_features = used_features \n",
        "\n",
        "    def forward(self, user, song,  energy, loudness,danceability= None, speechiness= None ,  tempo= None):\n",
        "        # TODO : Embed user and song\n",
        "        user_embed = self.user_embed(...)\n",
        "        song_embed = self.song_embed(...)\n",
        "        \n",
        "        # TODO : Perform the basic operation. Hint : Don't forget to unsqueeze some things if shapes doesn't fit..\n",
        "        x = ...\n",
        "        \n",
        "        # TODO : Pass through MLP layers\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        \n",
        "        # Output final prediction\n",
        "        return x.view(-1)\n",
        "        \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # TODO : Retrieve User, Song, Energy, Loudness, Rating from the input batch\n",
        "        user = ...\n",
        "        song = ...\n",
        "        energy = ...\n",
        "        loudness = ...\n",
        "        rating = ...        \n",
        "        # TODO : Forward pass\n",
        "        output = self.forward(...,...,...,...)        \n",
        "        # TODO : Compute your loss. Hint : Do we use a MSE, BCE, CE, Focal Loss ?.\n",
        "        loss = ...\n",
        "        # Log training loss\n",
        "        self.log('train_loss', loss)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # TODO : What changes do we have to make from the training step ? Can we just copy paste stuff ?\n",
        "        # TODO : Add what's needed.. You've done this at least 3 times now\n",
        "        ...\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # TODO : Define your learning_rate\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr= ...)\n",
        "        return optimizer\n",
        "\n"
      ],
      "metadata": {
        "id": "SVTYHc3ce19S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instantiate the model\n",
        "\n",
        "Now let's create an instance of the model and the datamodule."
      ],
      "metadata": {
        "id": "Ni2hZT8KZdq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TODO : Initialize model\n",
        "model = NCF(...,...)\n",
        "\n",
        "# TODO : Initialize datamodule\n",
        "datamodule = PlaylistDataModule(...,...)"
      ],
      "metadata": {
        "id": "wy5BKJRcZi5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainer : The magic of Lightning\n",
        "\n",
        "So we have the choice to either recreate the famous training, valid and testing loop, but meh..\n",
        "\n",
        "Lightning comes with a Trainer class that handles all the training for your.\n",
        "* By reading the documentation: https://lightning.ai/docs/pytorch/stable/common/trainer.html, initiate the attributes of the class.\n",
        "* Train and test the model on the dataset."
      ],
      "metadata": {
        "id": "e5n7qB-xZqSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : Initialize PyTorch Lightning Trainer\n",
        "trainer = pl.Trainer(accelerator=...,\n",
        "                     devices=...,\n",
        "                     max_epochs=...,\n",
        "                     fast_dev_run=...)\n",
        "\n",
        "# TODO : Train the model\n",
        "datamodule.setup(...) # TODO : Setup your datamodule to the training mode\n",
        ".. # TODO : TRAIN !!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "... # TODO : TEST !!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
      ],
      "metadata": {
        "id": "FfD_6g-EgKBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monitoring the Training\n",
        "\n",
        "Obviously, we have tools that allows us to monitor the training of our model. \n",
        "* Initiate a Tensorboard and watch your training.\n",
        "\n",
        "If you have issues showing the Tensorboard UX, you're cookies might not allow it. So try to activate them."
      ],
      "metadata": {
        "id": "1NjcbGjGfYr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir ..."
      ],
      "metadata": {
        "id": "aQjRqgdBjJVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the predictions\n",
        "\n",
        "* Create a Playlist for user 1 based on the ratings your model predicted. You can obviously retrieve the top10 songs. But how ? :)\n",
        "\n",
        "* The playlist should return the name of the song and its rating"
      ],
      "metadata": {
        "id": "5QNsD-3ua1u2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, user_id, song_ids):\n",
        "    ratings = ...            \n",
        "    return ratings.numpy()\n",
        "\n",
        "# TODO : Retrieve the TOP10 songs for user 1."
      ],
      "metadata": {
        "id": "Gd8AFVu6fZ4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III - Doing Some Feature Engineering : The Complete Package (Optional)\n",
        "\n",
        "As you might have seen, we only kept two features (Energy, Loudness) to compute the final Rating . But, we can use much more than that. As we have remaining features (Danceability..), we can add them to our rating computation, and explain their contribution on the prediction of our model.\n",
        "* Explain the impact of the other features on the training of the model.\n",
        "* Train (multiple?) models to validate your hypothesis.\n"
      ],
      "metadata": {
        "id": "SoT2xJCyWcu2"
      }
    }
  ]
}